{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "\n",
    "def test_single_file(file_path: str):\n",
    "    \"\"\"\n",
    "    Test processing of a single document\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the document file to test\n",
    "    \"\"\"\n",
    "    # API endpoint\n",
    "    url = \"http://localhost:8000/process\"\n",
    "    \n",
    "    # Check file exists\n",
    "    file_path = Path(file_path)\n",
    "    if not file_path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    # logger.info(f\"Testing file: {file_path.name}\")\n",
    "    \n",
    "    # Prepare file for request\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        files = [(\"files\", f)]\n",
    "        \n",
    "        try:\n",
    "            # Send request\n",
    "            # logger.info(\"Sending request to API...\")\n",
    "            response = requests.post(url, files=files)\n",
    "            \n",
    "            # Check response\n",
    "            assert response.status_code == 200, f\"API request failed with status code: {response.status_code}\"\n",
    "            \n",
    "            # Parse results\n",
    "            results = response.json()\n",
    "            return results\n",
    "            \n",
    "            # logger.info(\"Test completed successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            # logger.error(f\"Error during testing: {str(e)}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=test_single_file(\"/Users/frank/PycharmProjects/myOmniDocParser/sampledocs/è°·æ­Œå‘å¸ƒSigLIP 2ï¼šå¤šè¯­è¨€è§†è§‰-è¯­è¨€ç¼–ç å™¨çš„é©å‘½æ€§è¿›æ­¥.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'è°·æ­Œå‘å¸ƒSigLIP 2ï¼šå¤šè¯­è¨€è§†è§‰-è¯­è¨€ç¼–ç å™¨çš„é©å‘½æ€§è¿›æ­¥.pdf': [{'content': '2/22/25, 8:11 AM è°·æ­Œå‘å¸ƒ SigLIP 2 ï¼šå¤šè¯­è¨€è§†è§‰ - è¯­è¨€ç¼–ç å™¨çš„é©å‘½æ€§è¿›æ­¥\\nè°·æ­Œå‘å¸ƒSigLIP 2ï¼šå¤šè¯­è¨€è§†è§‰-è¯­è¨€ç¼–ç å™¨çš„é©å‘½æ€§è¿›æ­¥\\nInternLM3ç­‰LLM 2025å¹´02æœˆ22æ—¥ 06:48 æœºæ™ºæµ\\n<image id=\"001\">\\nIt appears that the image is blank or contains no visible content. Therefore, there is no information to extract or analyze. If you have another image or specific content you\\'d like me to look at, please share!\\n</image>\\nä½œè€…ï¼š InternLM3 ç­‰ LLM ï¼ˆå†…å®¹å¯èƒ½æœ‰è¯¯ï¼Œè¯·ä»”ç»†ç”„åˆ«ï¼‰\\nå…¨æ–‡çº¦  2400  å­—ï¼Œé¢„è®¡é˜…è¯»æ—¶é—´  6  åˆ†é’Ÿ\\nè®ºæ–‡é“¾æ¥ï¼š https://huggingface.co/papers/2502.14786\\nåœ¨äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVision-Language Models, VLMsï¼‰çš„æŠ€æœ¯è¿›æ­¥æ—¥æ–°æœˆå¼‚ã€‚æœ€\\nè¿‘ï¼ŒGoogle DeepMindæ¨å‡ºäº†ä¸€æ¬¾åä¸ºSigLIP 2çš„æ–°å¤šè¯­è¨€è§†è§‰-è¯­è¨€ç¼–ç å™¨ã€‚è¿™æ¬¾æ¨¡å‹åœ¨åŸæœ‰çš„\\nSigLIPåŸºç¡€ä¸Šè¿›è¡Œäº†å¤§å¹…ä¼˜åŒ–ï¼Œä¸ä»…åœ¨æ ¸å¿ƒä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¿˜æ‰©å±•äº†å¤šè¯­è¨€æ”¯æŒã€å¯†é›†é¢„æµ‹å’Œå®š\\nä½èƒ½åŠ›ã€‚ä»Šå¤©ï¼Œæˆ‘ä»¬å°†ä»¥ç¬¬ä¸‰æ–¹çš„è§†è§’ï¼Œä¸ºå¤§å®¶è¯¦ç»†è§£è¯»SigLIP 2çš„äº®ç‚¹å’Œä¼˜åŠ¿ã€‚\\nğŸŒŸ SigLIP 2æ˜¯ä»€ä¹ˆï¼Ÿæœ‰å“ªäº›çªç ´ï¼Ÿ\\nSigLIP 2æ˜¯SigLIPçš„å‡çº§ç‰ˆæœ¬ï¼Œå®ƒé€šè¿‡èåˆå¤šç§å…ˆè¿›æŠ€æœ¯ï¼ŒåŒ…æ‹¬åŸºäºå­—å¹•çš„é¢„è®­ç»ƒã€è‡ªç›‘ç£æŸå¤±\\nï¼ˆå¦‚è‡ªè’¸é¦ã€æ©ç é¢„æµ‹ï¼‰å’Œåœ¨çº¿æ•°æ®æ•´ç†ï¼Œæ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚ç›¸æ¯”å‰ä»£ï¼ŒSigLIP 2åœ¨é›¶-shotåˆ†\\nç±»ã€å›¾åƒ-æ–‡æœ¬æ£€ç´¢ä»¥åŠä½œä¸ºVLMsè§†è§‰ç¼–ç å™¨çš„è¿ç§»æ€§èƒ½ä¸Šéƒ½æœ‰äº†æ˜æ˜¾è¿›æ­¥ã€‚æ›´ä»¤äººå…´å¥‹çš„æ˜¯ï¼Œå®ƒ\\nåœ¨å®šä½ä»»åŠ¡å’Œå¯†é›†é¢„æµ‹ä»»åŠ¡ï¼ˆå¦‚åˆ†å‰²å’Œæ·±åº¦ä¼°è®¡ï¼‰ä¸Šä¹Ÿå®ç°äº†è´¨çš„é£è·ƒã€‚\\nğŸ” SigLIP 2çš„äº”å¤§æ ¸å¿ƒä¼˜åŠ¿\\n1. ğŸš€ å¼ºå¤§çš„å¤šè¯­è¨€æ”¯æŒ\\nSigLIP 2ä¸ä»…åœ¨ä»¥è‹±è¯­ä¸ºä¸»çš„è§†è§‰-è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¿˜èƒ½åœ¨å¤šè¯­è¨€åŸºå‡†æµ‹è¯•ä¸­æä¾›å¼ºåŠ²çš„ç»“\\næœã€‚è¿™ä½¿å¾—å®ƒèƒ½å¤Ÿæ— ç¼é€‚åº”å¤šç§è¯­è¨€å’Œæ–‡åŒ–åœºæ™¯ï¼Œä¸ºå…¨çƒåŒ–åº”ç”¨æ‰“å¼€äº†å¤§é—¨ã€‚\\nhttps://mp.weixin.qq.com/s/Ir1cC_lqF0m2LAeYlHI8og 1/6',\n",
       "   'metadata': {'source': '/var/folders/jz/pbm5_77s7951llp98zs6b3d80000gp/T/tmp65eb0on_/è°·æ­Œå‘å¸ƒSigLIP 2ï¼šå¤šè¯­è¨€è§†è§‰-è¯­è¨€ç¼–ç å™¨çš„é©å‘½æ€§è¿›æ­¥.pdf',\n",
       "    'page': 1,\n",
       "    'total_pages': 6,\n",
       "    'images': [{'bbox': [77.71660614013672,\n",
       "       103.67327880859375,\n",
       "       517.033447265625,\n",
       "       177.0009002685547],\n",
       "      'context': 'ğŸ” SigLIP 2çš„äº”å¤§æ ¸å¿ƒä¼˜åŠ¿\\n1. ğŸš€ å¼ºå¤§çš„å¤šè¯­è¨€æ”¯æŒ',\n",
       "      'extraction_status': 'success',\n",
       "      'extracted_content': \"It appears that the image is blank or contains no visible content. Therefore, there is no information to extract or analyze. If you have another image or specific content you'd like me to look at, please share!\"}]}},\n",
       "  {'content': '2/22/25, 8:11 AM è°·æ­Œå‘å¸ƒ SigLIP 2 ï¼šå¤šè¯­è¨€è§†è§‰ - è¯­è¨€ç¼–ç å™¨çš„é©å‘½æ€§è¿›æ­¥\\n<image id=\"002\">\\nThe image presents two bar graphs comparing the performance of three modelsâ€”SigLIP, SigLIP 2, and mSigLIPâ€”across two tasks: Text to Image Retrieval and Image to Text Retrieval.\\n\\n### Graph Details:\\n\\n1. **Top Graph (Text â†’ Image Retrieval)**:\\n   - **Y-Axis**: Recall (ranging from 0 to 1)\\n   - **X-Axis**: Different languages or categories (e.g., English, Spanish, Filipino, etc.)\\n   - **Bars**:\\n     - **SigLIP**: Represented in blue\\n     - **SigLIP 2**: Represented in orange\\n     - **mSigLIP**: Represented in green\\n   - **Trend**: The recall values for each model vary across different languages, with some languages showing higher recall than others.\\n\\n2. **Bottom Graph (Image â†’ Text Retrieval)**:\\n   - **Y-Axis**: Recall (ranging from 0 to 1)\\n   - **X-Axis**: Same languages or categories as the top graph.\\n   - **Bars**:\\n     - **SigLIP**: Blue\\n     - **SigLIP 2**: Orange\\n     - **mSigLIP**: Green\\n   - **Trend**: Similar to the top graph, the recall values differ across languages, indicating varying performance of the models.\\n\\n### Observations:\\n- Both graphs show a range of recall values, indicating the effectiveness of each model in retrieving relevant information based on the input type (text or image).\\n- The models exhibit different performance levels across various languages, suggesting that some languages may be more challenging for the models than others.\\n\\n### Conclusion:\\nThe graphs effectively illustrate the comparative performance of the three models in two retrieval tasks, highlighting the variability in recall across different languages.\\n</image>\\nå›¾2ï¼šSigLIPã€SigLIP 2å’ŒmSigLIPåœ¨Crossmodal-3600ä¸Šçš„æ¯ç§è¯­è¨€å›¾åƒ-æ–‡æœ¬æ£€ç´¢æ€§èƒ½å¯¹æ¯”ã€‚\\nSigLIP 2åœ¨å¤šè¯­è¨€æ£€ç´¢ä¸­å‡ ä¹è¿½å¹³mSigLIPï¼ŒåŒæ—¶åœ¨è‹±è¯­ä»»åŠ¡ä¸Šè¡¨ç°æ›´ä¼˜ã€‚\\n2. ğŸ–¼ ï¸  å¯†é›†ç‰¹å¾çš„æå‡\\né€šè¿‡ç»“åˆè‡ªç›‘ç£æŸå¤±å’ŒåŸºäºè§£ç å™¨çš„æŸå¤±ï¼ŒSigLIP 2åœ¨å¯†é›†é¢„æµ‹ä»»åŠ¡ï¼ˆå¦‚è¯­ä¹‰åˆ†å‰²ã€æ·±åº¦ä¼°è®¡ï¼‰å’Œ\\nå®šä½ä»»åŠ¡ï¼ˆå¦‚æŒ‡ä»£è¡¨è¾¾å¼ç†è§£ï¼‰ä¸Šè¡¨ç°æ›´ä½³ã€‚è¿™ç§èƒ½åŠ›å¯¹äºéœ€è¦ç²¾ç»†è§†è§‰ç†è§£çš„åº”ç”¨è‡³å…³é‡è¦ã€‚\\nhttps://mp.weixin.qq.com/s/Ir1cC_lqF0m2LAeYlHI8og 2/6',\n",
       "   'metadata': {'source': '/var/folders/jz/pbm5_77s7951llp98zs6b3d80000gp/T/tmp65eb0on_/è°·æ­Œå‘å¸ƒSigLIP 2ï¼šå¤šè¯­è¨€è§†è§‰-è¯­è¨€ç¼–ç å™¨çš„é©å‘½æ€§è¿›æ­¥.pdf',\n",
       "    'page': 2,\n",
       "    'total_pages': 6,\n",
       "    'images': [{'bbox': [77.71660614013672,\n",
       "       27.75,\n",
       "       517.033447265625,\n",
       "       203.60650634765625],\n",
       "      'context': '2. ğŸ–¼ ï¸  å¯†é›†ç‰¹å¾çš„æå‡',\n",
       "      'extraction_status': 'success',\n",
       "      'extracted_content': 'The image presents two bar graphs comparing the performance of three modelsâ€”SigLIP, SigLIP 2, and mSigLIPâ€”across two tasks: Text to Image Retrieval and Image to Text Retrieval.\\n\\n### Graph Details:\\n\\n1. **Top Graph (Text â†’ Image Retrieval)**:\\n   - **Y-Axis**: Recall (ranging from 0 to 1)\\n   - **X-Axis**: Different languages or categories (e.g., English, Spanish, Filipino, etc.)\\n   - **Bars**:\\n     - **SigLIP**: Represented in blue\\n     - **SigLIP 2**: Represented in orange\\n     - **mSigLIP**: Represented in green\\n   - **Trend**: The recall values for each model vary across different languages, with some languages showing higher recall than others.\\n\\n2. **Bottom Graph (Image â†’ Text Retrieval)**:\\n   - **Y-Axis**: Recall (ranging from 0 to 1)\\n   - **X-Axis**: Same languages or categories as the top graph.\\n   - **Bars**:\\n     - **SigLIP**: Blue\\n     - **SigLIP 2**: Orange\\n     - **mSigLIP**: Green\\n   - **Trend**: Similar to the top graph, the recall values differ across languages, indicating varying performance of the models.\\n\\n### Observations:\\n- Both graphs show a range of recall values, indicating the effectiveness of each model in retrieving relevant information based on the input type (text or image).\\n- The models exhibit different performance levels across various languages, suggesting that some languages may be more challenging for the models than others.\\n\\n### Conclusion:\\nThe graphs effectively illustrate the comparative performance of the three models in two retrieval tasks, highlighting the variability in recall across different languages.'}]}},\n",
       "  {'content': '2/22/25, 8:11 AM è°·æ­Œå‘å¸ƒ SigLIP 2 ï¼šå¤šè¯­è¨€è§†è§‰ - è¯­è¨€ç¼–ç å™¨çš„é©å‘½æ€§è¿›æ­¥\\n<image id=\"003\">\\nThe image contains a bar graph comparing various metrics across different datasets and models. Hereâ€™s a structured breakdown of the relevant information:\\n\\n### Datasets/Models:\\n1. **A2D**\\n2. **AOKVQA-DA (val)**\\n3. **AOKVQA-MC (val)**\\n4. **COCO-35L (avg34)**\\n5. **COCO-35L (en)**\\n6. **COCOCap**\\n7. **CountBenchQA**\\n8. **DocVQA (val)**\\n9. **GQA**\\n10. **InfoVQA (val)**\\n11. **NLVR2**\\n12. **NoCaps**\\n13. **OCR-VQA**\\n14. **OKVQA**\\n15. **RefCOCO (testA)**\\n16. **RefCOCO (testB)**\\n17. **RefCOCO+ (val)**\\n18. **RefCOCO+ (testA)**\\n19. **RefCOCO+ (testB)**\\n20. **RefCOCOg (test)**\\n21. **RefCOCOg (val)**\\n22. **ST-VQA (val)**\\n23. **SciCap**\\n24. **ScienceQA**\\n25. **Screen2Words**\\n26. **TallyQA (complex)**\\n27. **TallyQA (simple)**\\n28. **TextCaps**\\n29. **TextVQA (val)**\\n30. **VQA2 (minival)**\\n31. **VizWizVQA (val)**\\n32. **WidgetCap**\\n33. **XM3600 (avg35)**\\n34. **XM3600 (en)**\\n35. **Average**\\n\\n### Metrics:\\n- The bars represent different configurations or models:\\n  - **SigLIP L/16 256px** (light blue)\\n  - **SigLIP L/16 256px** (orange)\\n  - **SigLIP S/400m/14 224px** (green)\\n  - **SigLIP S/400m/14 224px** (dark blue)\\n  - **SigLIP S/400m/14 384px** (yellow)\\n\\n### Observations:\\n- Each dataset/model has a corresponding bar for each configuration, indicating performance metrics (likely accuracy or some evaluation score).\\n- The graph appears to show comparative performance across different models and datasets, with variations in height indicating differences in performance.\\n\\n### Conclusion:\\nThis image presents a comparative analysis of various visual question answering (VQA) models across multiple datasets, highlighting their performance metrics in a clear bar graph format.\\n</image>\\nè¡¨2ï¼šSigLIP 2åœ¨å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨è¯­ä¹‰åˆ†å‰²ã€æ·±åº¦ä¼°è®¡å’Œè¡¨é¢æ³•çº¿ä¼°è®¡ä¸Šçš„ä¼˜\\nå¼‚è¡¨ç°ã€‚\\n3. ğŸ”„ å‘åå…¼å®¹æ€§\\nSigLIP 2æ²¿ç”¨äº†SigLIPçš„æ¶æ„è®¾è®¡ï¼Œç”¨æˆ·åªéœ€æ›¿æ¢æ¨¡å‹æƒé‡å’Œtokenizerï¼ˆç°å·²å‡çº§ä¸ºå¤šè¯­è¨€ç‰ˆ\\næœ¬ï¼‰ï¼Œå³å¯äº«å—åˆ°æ€§èƒ½æå‡ã€‚è¿™ç§è®¾è®¡æå¤§åœ°æ–¹ä¾¿äº†ç°æœ‰ç”¨æˆ·çš„è¿ç§»ã€‚\\n4. ğŸ“ åŸç”Ÿå®½é«˜æ¯”ä¸å¯å˜åˆ†è¾¨ç‡\\nSigLIP 2æ¨å‡ºäº†NaFlexå˜ä½“ï¼Œæ”¯æŒå¤šç§åˆ†è¾¨ç‡å¹¶ä¿æŒå›¾åƒçš„åŸç”Ÿå®½é«˜æ¯”ã€‚è¿™å¯¹äºæ–‡æ¡£ç†è§£ç­‰å¯¹å®½é«˜\\næ¯”æ•æ„Ÿçš„ä»»åŠ¡å°¤ä¸ºé‡è¦ï¼Œæå‡äº†æ¨¡å‹çš„çµæ´»æ€§å’Œå®ç”¨æ€§ã€‚\\nhttps://mp.weixin.qq.com/s/Ir1cC_lqF0m2LAeYlHI8og 3/6',\n",
       "   'metadata': {'source': '/var/folders/jz/pbm5_77s7951llp98zs6b3d80000gp/T/tmp65eb0on_/è°·æ­Œå‘å¸ƒSigLIP 2ï¼šå¤šè¯­è¨€è§†è§‰-è¯­è¨€ç¼–ç å™¨çš„é©å‘½æ€§è¿›æ­¥.pdf',\n",
       "    'page': 3,\n",
       "    'total_pages': 6,\n",
       "    'images': [{'bbox': [77.71660614013672,\n",
       "       27.75,\n",
       "       517.033447265625,\n",
       "       499.51263427734375],\n",
       "      'context': '3. ğŸ”„ å‘åå…¼å®¹æ€§\\n4. ğŸ“ åŸç”Ÿå®½é«˜æ¯”ä¸å¯å˜åˆ†è¾¨ç‡',\n",
       "      'extraction_status': 'success',\n",
       "      'extracted_content': 'The image contains a bar graph comparing various metrics across different datasets and models. Hereâ€™s a structured breakdown of the relevant information:\\n\\n### Datasets/Models:\\n1. **A2D**\\n2. **AOKVQA-DA (val)**\\n3. **AOKVQA-MC (val)**\\n4. **COCO-35L (avg34)**\\n5. **COCO-35L (en)**\\n6. **COCOCap**\\n7. **CountBenchQA**\\n8. **DocVQA (val)**\\n9. **GQA**\\n10. **InfoVQA (val)**\\n11. **NLVR2**\\n12. **NoCaps**\\n13. **OCR-VQA**\\n14. **OKVQA**\\n15. **RefCOCO (testA)**\\n16. **RefCOCO (testB)**\\n17. **RefCOCO+ (val)**\\n18. **RefCOCO+ (testA)**\\n19. **RefCOCO+ (testB)**\\n20. **RefCOCOg (test)**\\n21. **RefCOCOg (val)**\\n22. **ST-VQA (val)**\\n23. **SciCap**\\n24. **ScienceQA**\\n25. **Screen2Words**\\n26. **TallyQA (complex)**\\n27. **TallyQA (simple)**\\n28. **TextCaps**\\n29. **TextVQA (val)**\\n30. **VQA2 (minival)**\\n31. **VizWizVQA (val)**\\n32. **WidgetCap**\\n33. **XM3600 (avg35)**\\n34. **XM3600 (en)**\\n35. **Average**\\n\\n### Metrics:\\n- The bars represent different configurations or models:\\n  - **SigLIP L/16 256px** (light blue)\\n  - **SigLIP L/16 256px** (orange)\\n  - **SigLIP S/400m/14 224px** (green)\\n  - **SigLIP S/400m/14 224px** (dark blue)\\n  - **SigLIP S/400m/14 384px** (yellow)\\n\\n### Observations:\\n- Each dataset/model has a corresponding bar for each configuration, indicating performance metrics (likely accuracy or some evaluation score).\\n- The graph appears to show comparative performance across different models and datasets, with variations in height indicating differences in performance.\\n\\n### Conclusion:\\nThis image presents a comparative analysis of various visual question answering (VQA) models across multiple datasets, highlighting their performance metrics in a clear bar graph format.'}]}},\n",
       "  {'content': '2/22/25, 8:11 AM è°·æ­Œå‘å¸ƒ SigLIP 2 ï¼šå¤šè¯­è¨€è§†è§‰ - è¯­è¨€ç¼–ç å™¨çš„é©å‘½æ€§è¿›æ­¥\\n<image id=\"004\">\\nThe image contains a series of graphs comparing different models across various tasks, focusing on performance metrics as a function of sequence length. Hereâ€™s a breakdown of the relevant information:\\n\\n### Graph Titles:\\n1. **INet 0-shot**\\n2. **INet v2 0-shot**\\n3. **INet ReaL 0-shot**\\n4. **ObjectNet 0-shot**\\n5. **COCO R@1 â†” T**\\n6. **TextCaps R@1 â†” I**\\n7. **TextCaps R@1 â†” T**\\n8. **HierText R@1 â†” I**\\n9. **HierText R@1 â†” T**\\n10. **SciCap R@1 â†” I**\\n11. **SciCap R@1 â†” T**\\n12. **Screen2Words R@1 â†” I**\\n13. **Screen2Words R@1 â†” T**\\n\\n### Axes:\\n- **X-axis:** Sequence length (values: 64, 256, 576, 784, 1024)\\n- **Y-axis:** Performance metric (percentage, ranging from 0 to 100)\\n\\n### Models:\\n- **SigLIP 2 (NaFlex)**: Represented by a blue line.\\n- **SigLIP 2**: Represented by an orange line.\\n- **ViT**: Indicated by a specific marker style (not detailed in the image).\\n\\n### Additional Markers:\\n- **B/16**: Indicated by a specific marker style (not detailed in the image).\\n- **So400m/16**: Indicated by a specific marker style (not detailed in the image).\\n\\n### Observations:\\n- The graphs show trends in performance for different models as the sequence length increases.\\n- Each graph compares the performance of the models on specific tasks, with varying results based on the model and task.\\n- The performance generally increases with longer sequence lengths, but the rate of increase varies by model and task.\\n\\nThis analysis captures the key elements and relationships presented in the image.\\n</image>\\nå›¾3ï¼šNaFlexå˜ä½“ä¸æ ‡å‡†SigLIP 2åœ¨ä¸åŒåºåˆ—é•¿åº¦ä¸‹çš„æ€§èƒ½æ¯”è¾ƒï¼Œå±•ç¤ºäº†NaFlexåœ¨OCRå’Œæ–‡æ¡£ç›¸å…³\\nä»»åŠ¡ä¸Šçš„ä¼˜åŠ¿ã€‚\\n5. ğŸ’ª å¼ºå¤§çš„å°æ¨¡å‹\\né€šè¿‡ä¸»åŠ¨æ•°æ®æ•´ç†æŠ€æœ¯ï¼ŒSigLIP 2ä¼˜åŒ–äº†è¾ƒå°æ¨¡å‹ï¼ˆå¦‚B/16å’ŒB/32ï¼‰çš„æ€§èƒ½ã€‚å³ä½¿åœ¨èµ„æºå—é™çš„\\nåœºæ™¯ä¸‹ï¼Œè¿™äº›å°æ¨¡å‹ä¹Ÿèƒ½ä¿æŒå‡ºè‰²çš„è¡¨ç°ã€‚\\nğŸ“Š SigLIP 2çš„æ€§èƒ½æ•°æ®\\nSigLIP 2åœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†SigLIPå’Œå…¶ä»–å¼€æºæ¨¡å‹ã€‚ä»¥ä¸‹æ˜¯å®ƒåœ¨é›¶-shotåˆ†ç±»å’Œæ£€ç´¢ä»»åŠ¡ä¸­\\nçš„è¡¨ç°ï¼š\\n<image id=\"005\">\\nThe image presents two bar graphs comparing the performance of three modelsâ€”SigLIP, SigLIP 2, and mSigLIPâ€”across two tasks: Text to Image Retrieval and Image to Text Retrieval.\\n\\n### Graph Details:\\n\\n1. **Top Graph (Text â†’ Image Retrieval)**:\\n   - **Y-Axis**: Recall (ranging from 0 to 1)\\n   - **X-Axis**: Different languages or categories (e.g., English, Spanish, Filipino, etc.)\\n   - **Bars**:\\n     - **SigLIP**: Represented in blue\\n     - **SigLIP 2**: Represented in orange\\n     - **mSigLIP**: Represented in green\\n   - **Trend**: The recall values for each model vary across different languages, with some languages showing higher recall than others.\\n\\n2. **Bottom Graph (Image â†’ Text Retrieval)**:\\n   - **Y-Axis**: Recall (ranging from 0 to 1)\\n   - **X-Axis**: Different languages or categories (e.g., Arabic, Hungarian, English, etc.)\\n   - **Bars**:\\n     - **SigLIP**: Represented in blue\\n     - **SigLIP 2**: Represented in orange\\n     - **mSigLIP**: Represented in green\\n   - **Trend**: Similar to the top graph, the recall values differ across languages, indicating varying performance of the models.\\n\\n### Observations:\\n- The models exhibit different levels of effectiveness in retrieving images based on text and vice versa.\\n- Certain languages may yield higher recall rates, suggesting that the models perform better with specific languages or datasets.\\n\\n### Conclusion:\\nThe graphs provide a comparative analysis of the three models\\' performance in text-to-image and image-to-text retrieval tasks, highlighting the variability in recall across different languages.\\n</image>\\nå›¾4ï¼šSigLIP 2ä¸å¤šä¸ªåŸºçº¿æ¨¡å‹åœ¨ImageNet-1kã€COCOã€Flickrå’ŒXM3600ä¸Šçš„æ€§èƒ½å¯¹æ¯”ã€‚SigLIP\\n2åœ¨æ‰€æœ‰æ¨¡å‹è§„æ¨¡ä¸Šå‡ä¼˜äºSigLIPï¼Œå°¤å…¶åœ¨å°æ¨¡å‹ä¸Šè¿›æ­¥æ˜¾è‘—ã€‚\\nhttps://mp.weixin.qq.com/s/Ir1cC_lqF0m2LAeYlHI8og 4/6',\n",
       "   'metadata': {'source': '/var/folders/jz/pbm5_77s7951llp98zs6b3d80000gp/T/tmp65eb0on_/è°·æ­Œå‘å¸ƒSigLIP 2ï¼šå¤šè¯­è¨€è§†è§‰-è¯­è¨€ç¼–ç å™¨çš„é©å‘½æ€§è¿›æ­¥.pdf',\n",
       "    'page': 4,\n",
       "    'total_pages': 6,\n",
       "    'images': [{'bbox': [77.71660614013672,\n",
       "       27.75,\n",
       "       517.033447265625,\n",
       "       293.1570739746094],\n",
       "      'context': '5. ğŸ’ª å¼ºå¤§çš„å°æ¨¡å‹\\nğŸ“Š SigLIP 2çš„æ€§èƒ½æ•°æ®',\n",
       "      'extraction_status': 'success',\n",
       "      'extracted_content': 'The image contains a series of graphs comparing different models across various tasks, focusing on performance metrics as a function of sequence length. Hereâ€™s a breakdown of the relevant information:\\n\\n### Graph Titles:\\n1. **INet 0-shot**\\n2. **INet v2 0-shot**\\n3. **INet ReaL 0-shot**\\n4. **ObjectNet 0-shot**\\n5. **COCO R@1 â†” T**\\n6. **TextCaps R@1 â†” I**\\n7. **TextCaps R@1 â†” T**\\n8. **HierText R@1 â†” I**\\n9. **HierText R@1 â†” T**\\n10. **SciCap R@1 â†” I**\\n11. **SciCap R@1 â†” T**\\n12. **Screen2Words R@1 â†” I**\\n13. **Screen2Words R@1 â†” T**\\n\\n### Axes:\\n- **X-axis:** Sequence length (values: 64, 256, 576, 784, 1024)\\n- **Y-axis:** Performance metric (percentage, ranging from 0 to 100)\\n\\n### Models:\\n- **SigLIP 2 (NaFlex)**: Represented by a blue line.\\n- **SigLIP 2**: Represented by an orange line.\\n- **ViT**: Indicated by a specific marker style (not detailed in the image).\\n\\n### Additional Markers:\\n- **B/16**: Indicated by a specific marker style (not detailed in the image).\\n- **So400m/16**: Indicated by a specific marker style (not detailed in the image).\\n\\n### Observations:\\n- The graphs show trends in performance for different models as the sequence length increases.\\n- Each graph compares the performance of the models on specific tasks, with varying results based on the model and task.\\n- The performance generally increases with longer sequence lengths, but the rate of increase varies by model and task.\\n\\nThis analysis captures the key elements and relationships presented in the image.'},\n",
       "     {'bbox': [77.71660614013672,\n",
       "       548.1815185546875,\n",
       "       517.033447265625,\n",
       "       724.0380249023438],\n",
       "      'context': 'SigLIP 2åœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†SigLIPå’Œå…¶ä»–å¼€æºæ¨¡å‹ã€‚ä»¥ä¸‹æ˜¯å®ƒåœ¨é›¶-shotåˆ†ç±»å’Œæ£€ç´¢ä»»åŠ¡ä¸­\\nçš„è¡¨ç°ï¼š',\n",
       "      'extraction_status': 'success',\n",
       "      'extracted_content': \"The image presents two bar graphs comparing the performance of three modelsâ€”SigLIP, SigLIP 2, and mSigLIPâ€”across two tasks: Text to Image Retrieval and Image to Text Retrieval.\\n\\n### Graph Details:\\n\\n1. **Top Graph (Text â†’ Image Retrieval)**:\\n   - **Y-Axis**: Recall (ranging from 0 to 1)\\n   - **X-Axis**: Different languages or categories (e.g., English, Spanish, Filipino, etc.)\\n   - **Bars**:\\n     - **SigLIP**: Represented in blue\\n     - **SigLIP 2**: Represented in orange\\n     - **mSigLIP**: Represented in green\\n   - **Trend**: The recall values for each model vary across different languages, with some languages showing higher recall than others.\\n\\n2. **Bottom Graph (Image â†’ Text Retrieval)**:\\n   - **Y-Axis**: Recall (ranging from 0 to 1)\\n   - **X-Axis**: Different languages or categories (e.g., Arabic, Hungarian, English, etc.)\\n   - **Bars**:\\n     - **SigLIP**: Represented in blue\\n     - **SigLIP 2**: Represented in orange\\n     - **mSigLIP**: Represented in green\\n   - **Trend**: Similar to the top graph, the recall values differ across languages, indicating varying performance of the models.\\n\\n### Observations:\\n- The models exhibit different levels of effectiveness in retrieving images based on text and vice versa.\\n- Certain languages may yield higher recall rates, suggesting that the models perform better with specific languages or datasets.\\n\\n### Conclusion:\\nThe graphs provide a comparative analysis of the three models' performance in text-to-image and image-to-text retrieval tasks, highlighting the variability in recall across different languages.\"}]}},\n",
       "  {'content': '2/22/25, 8:11 AM è°·æ­Œå‘å¸ƒ SigLIP 2 ï¼šå¤šè¯­è¨€è§†è§‰ - è¯­è¨€ç¼–ç å™¨çš„é©å‘½æ€§è¿›æ­¥\\nğŸŒ æ–‡åŒ–å¤šæ ·æ€§ä¸å…¬å¹³æ€§\\nSigLIP 2ä¸ä»…åœ¨æ€§èƒ½ä¸Šæ›´è¿›ä¸€æ­¥ï¼Œè¿˜åœ¨æ–‡åŒ–åŒ…å®¹æ€§å’Œå…¬å¹³æ€§ä¸Šè¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚å®ƒé€šè¿‡è®­ç»ƒæ•°æ®ä¸­\\nçš„å¤šè¯­è¨€æ”¯æŒå’Œå»åæŠ€æœ¯ï¼Œåœ¨åœ°ç†å¤šæ ·åŒ–çš„ç‰©ä½“åˆ†ç±»å’Œåœ°ç†å®šä½ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†\\nè¡¨ç¤ºåå·®ã€‚\\n<image id=\"006\">\\nThe image presents two bar graphs comparing performance metrics across different models in two scenarios: \"10-shot\" and \"0-shot.\"\\n\\n### Graph Details:\\n\\n#### 10-shot Graph (Left):\\n- **X-axis Categories**:\\n  - Dollar Street\\n  - GeoDE (country)\\n  - GeoDE (region)\\n  \\n- **Y-axis**: Performance metric (values range from 0 to 40)\\n\\n- **Bars**:\\n  - **SigLIP B/16**: Light blue\\n  - **SigLIP 2 B/16**: Dark blue\\n  - **SigLIP L/16**: Light brown\\n  - **SigLIP 2 L/16**: Dark brown\\n\\n#### 0-shot Graph (Right):\\n- **X-axis Categories**:\\n  - Dollar Street\\n  - GLDv2\\n  - GeoDE\\n  \\n- **Y-axis**: Performance metric (values range from 0 to 100)\\n\\n- **Bars**:\\n  - **SigLIP B/16**: Light blue\\n  - **SigLIP 2 B/16**: Dark blue\\n  - **SigLIP L/16**: Light brown\\n  - **SigLIP 2 L/16**: Dark brown\\n\\n### Observations:\\n- In the **10-shot** scenario, the highest performance is observed for **GeoDE (region)** with **SigLIP 2 L/16**.\\n- In the **0-shot** scenario, **GeoDE** shows the highest performance, particularly with **SigLIP B/16**.\\n- The performance metrics differ significantly between the two scenarios, indicating varying effectiveness of the models based on the shot type.\\n\\n### Conclusion:\\nThe graphs illustrate the comparative performance of different models under varying conditions, highlighting the effectiveness of specific configurations in both 10-shot and 0-shot learning scenarios.\\n</image>\\nå›¾5ï¼šSigLIP 2åœ¨Dollar Streetã€GeoDEå’ŒGLDv2ä¸Šçš„10-shotå’Œ0-shotå‡†ç¡®ç‡ï¼Œå±•ç¤ºäº†å…¶åœ¨æ–‡åŒ–\\nå¤šæ ·æ€§ä»»åŠ¡ä¸Šçš„ä¼˜å¼‚è¡¨ç°ã€‚\\n<image id=\"007\">\\nThe image is a bar chart displaying data related to \"Representation bias\" for different categories labeled as follows:\\n\\n### Categories:\\n1. **SigLIP B/16**\\n   - Color: Light Blue\\n   - Value: Approximately 30\\n\\n2. **SigLIP 2 B/16**\\n   - Color: Dark Blue\\n   - Value: Approximately 20\\n\\n3. **SigLIP L/16**\\n   - Color: Light Brown\\n   - Value: Approximately 25\\n\\n4. **SigLIP 2 L/16**\\n   - Color: Dark Brown\\n   - Value: Approximately 10\\n\\n### X-Axis:\\n- Label: **Representation bias**\\n- Range: 0 to 35\\n\\n### Y-Axis:\\n- Categories listed vertically.\\n\\n### Observations:\\n- The highest representation bias is observed in **SigLIP B/16**.\\n- The lowest representation bias is in **SigLIP 2 L/16**.\\n- The chart visually compares the representation bias across the four categories.\\n</image>\\nå›¾6ï¼šä¸åŒæ¨¡å‹çš„è¡¨ç¤ºåå·®å¯¹æ¯”ï¼ŒSigLIP 2æ˜¾è‘—é™ä½äº†åå·®ï¼Œæå‡äº†æ¨¡å‹çš„å…¬å¹³æ€§ã€‚\\nğŸ¯ æ€»ç»“\\nSigLIP 2çš„å‘å¸ƒæ ‡å¿—ç€å¤šè¯­è¨€è§†è§‰-è¯­è¨€ç¼–ç å™¨çš„ä¸€æ¬¡é‡è¦é£è·ƒã€‚å®ƒåœ¨æ€§èƒ½ã€å…¼å®¹æ€§ã€å¤šè¯­è¨€æ”¯æŒ\\nå’Œå…¬å¹³æ€§ä¸Šçš„å…¨é¢æå‡ï¼Œä½¿å…¶æˆä¸ºè§†è§‰-è¯­è¨€ä»»åŠ¡çš„ç†æƒ³é€‰æ‹©ã€‚å¯¹äºç ”ç©¶äººå‘˜å’Œå¼€å‘è€…æ¥è¯´ï¼Œ\\nhttps://mp.weixin.qq.com/s/Ir1cC_lqF0m2LAeYlHI8og 5/6',\n",
       "   'metadata': {'source': '/var/folders/jz/pbm5_77s7951llp98zs6b3d80000gp/T/tmp65eb0on_/è°·æ­Œå‘å¸ƒSigLIP 2ï¼šå¤šè¯­è¨€è§†è§‰-è¯­è¨€ç¼–ç å™¨çš„é©å‘½æ€§è¿›æ­¥.pdf',\n",
       "    'page': 5,\n",
       "    'total_pages': 6,\n",
       "    'images': [{'bbox': [78.36552429199219,\n",
       "       132.22572326660156,\n",
       "       516.384521484375,\n",
       "       371.67608642578125],\n",
       "      'context': 'çš„å¤šè¯­è¨€æ”¯æŒå’Œå»åæŠ€æœ¯ï¼Œåœ¨åœ°ç†å¤šæ ·åŒ–çš„ç‰©ä½“åˆ†ç±»å’Œåœ°ç†å®šä½ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†\\nè¡¨ç¤ºåå·®ã€‚',\n",
       "      'extraction_status': 'success',\n",
       "      'extracted_content': 'The image presents two bar graphs comparing performance metrics across different models in two scenarios: \"10-shot\" and \"0-shot.\"\\n\\n### Graph Details:\\n\\n#### 10-shot Graph (Left):\\n- **X-axis Categories**:\\n  - Dollar Street\\n  - GeoDE (country)\\n  - GeoDE (region)\\n  \\n- **Y-axis**: Performance metric (values range from 0 to 40)\\n\\n- **Bars**:\\n  - **SigLIP B/16**: Light blue\\n  - **SigLIP 2 B/16**: Dark blue\\n  - **SigLIP L/16**: Light brown\\n  - **SigLIP 2 L/16**: Dark brown\\n\\n#### 0-shot Graph (Right):\\n- **X-axis Categories**:\\n  - Dollar Street\\n  - GLDv2\\n  - GeoDE\\n  \\n- **Y-axis**: Performance metric (values range from 0 to 100)\\n\\n- **Bars**:\\n  - **SigLIP B/16**: Light blue\\n  - **SigLIP 2 B/16**: Dark blue\\n  - **SigLIP L/16**: Light brown\\n  - **SigLIP 2 L/16**: Dark brown\\n\\n### Observations:\\n- In the **10-shot** scenario, the highest performance is observed for **GeoDE (region)** with **SigLIP 2 L/16**.\\n- In the **0-shot** scenario, **GeoDE** shows the highest performance, particularly with **SigLIP B/16**.\\n- The performance metrics differ significantly between the two scenarios, indicating varying effectiveness of the models based on the shot type.\\n\\n### Conclusion:\\nThe graphs illustrate the comparative performance of different models under varying conditions, highlighting the effectiveness of specific configurations in both 10-shot and 0-shot learning scenarios.'},\n",
       "     {'bbox': [121.19405364990234,\n",
       "       441.75909423828125,\n",
       "       473.5559997558594,\n",
       "       653.9549560546875],\n",
       "      'context': 'å›¾5ï¼šSigLIP 2åœ¨Dollar Streetã€GeoDEå’ŒGLDv2ä¸Šçš„10-shotå’Œ0-shotå‡†ç¡®ç‡ï¼Œå±•ç¤ºäº†å…¶åœ¨æ–‡åŒ–\\nå¤šæ ·æ€§ä»»åŠ¡ä¸Šçš„ä¼˜å¼‚è¡¨ç°ã€‚',\n",
       "      'extraction_status': 'success',\n",
       "      'extracted_content': 'The image is a bar chart displaying data related to \"Representation bias\" for different categories labeled as follows:\\n\\n### Categories:\\n1. **SigLIP B/16**\\n   - Color: Light Blue\\n   - Value: Approximately 30\\n\\n2. **SigLIP 2 B/16**\\n   - Color: Dark Blue\\n   - Value: Approximately 20\\n\\n3. **SigLIP L/16**\\n   - Color: Light Brown\\n   - Value: Approximately 25\\n\\n4. **SigLIP 2 L/16**\\n   - Color: Dark Brown\\n   - Value: Approximately 10\\n\\n### X-Axis:\\n- Label: **Representation bias**\\n- Range: 0 to 35\\n\\n### Y-Axis:\\n- Categories listed vertically.\\n\\n### Observations:\\n- The highest representation bias is observed in **SigLIP B/16**.\\n- The lowest representation bias is in **SigLIP 2 L/16**.\\n- The chart visually compares the representation bias across the four categories.'}]}},\n",
       "  {'content': '2/22/25, 8:11 AM è°·æ­Œå‘å¸ƒ SigLIP 2 ï¼šå¤šè¯­è¨€è§†è§‰ - è¯­è¨€ç¼–ç å™¨çš„é©å‘½æ€§è¿›æ­¥\\nSigLIP 2ä¸ä»…æä¾›äº†æ›´é«˜çš„æ€§èƒ½ï¼Œè¿˜å¸¦æ¥äº†æ›´å¹¿æ³›çš„åº”ç”¨å¯èƒ½æ€§ã€‚\\næœŸå¾…SigLIP 2åœ¨å¼€æºç¤¾åŒºä¸­æ¿€å‘æ›´å¤šåˆ›æ–°ç«èŠ±ï¼ğŸ”¥\\n-- å®Œ --\\næœºæ™ºæµ æ¨è é˜…è¯»ï¼š\\n1.\\xa0 æŒæ¡å¦‚ä½•æ­å»ºé«˜æ•ˆçš„å¤§æ¨¡å‹ä»»åŠ¡æµï¼ˆäºŒï¼‰ï¼šå¦‚ä½•ç”¨é“¾å¼æµç¨‹æå‡ AI å¤„ç†èƒ½åŠ›ï¼Ÿ\\n2.\\xa0 æŒæ¡å¦‚ä½•æ­å»ºé«˜æ•ˆçš„å¤§æ¨¡å‹ä»»åŠ¡æµï¼ˆä¸€ï¼‰ï¼šLangChainä»»åŠ¡æµæ„å»º\\n3.\\xa0 æ­ç§˜LangChainè®°å¿†æ¨¡å—ï¼šä¸‡å­—é•¿æ–‡è¯¦è§£å››ç§Conversation Memoryï¼ŒåŠ©åŠ›AIå¯¹è¯æ›´èª\\næ˜ï¼\\n4.\\xa0 ä¸‡å­—é•¿æ–‡ï¼æ‰‹æŠŠæ‰‹å¸¦ä½ ä¸Šæ‰‹åŸºäºLangChainåŠQwenå¤§æ¨¡å‹çš„å¼€å‘ä¸åº”ç”¨\\næ¬¢è¿åœ¨ â€œ æœºæ™ºæµ â€ å…¬ä¼—å·åå°å›å¤ â€œccâ€ ï¼ŒåŠ å…¥æœºæ™ºæµå¤§æ¨¡å‹äº¤æµç¾¤ ï¼Œä¸æˆ‘ä»¬ä¸€èµ·æ¢ç´¢  AI  ä¸äººç±»\\næ½œèƒ½çš„æœªæ¥ï¼Œä¸€èµ·å…±èµ´  AI  æµªæ½®ï¼\\n<image id=\"008\">\\nThe image features a logo that consists of a stylized design with two overlapping shapes in light blue and dark blue, set against a black background. Below the design, there is text in Chinese characters that reads \"æœºå™¨æµ,\" which translates to \"Machine Flow\" in English. \\n\\nIf you need further analysis or specific details, please let me know!\\n</image>\\næœºæ™ºæµ\\nå…±èµ´ AI æ—¶ä»£æµªæ½®~\\n476ç¯‡åŸåˆ›å†…å®¹\\nå…¬ä¼—å·\\nhttps://mp.weixin.qq.com/s/Ir1cC_lqF0m2LAeYlHI8og 6/6',\n",
       "   'metadata': {'source': '/var/folders/jz/pbm5_77s7951llp98zs6b3d80000gp/T/tmp65eb0on_/è°·æ­Œå‘å¸ƒSigLIP 2ï¼šå¤šè¯­è¨€è§†è§‰-è¯­è¨€ç¼–ç å™¨çš„é©å‘½æ€§è¿›æ­¥.pdf',\n",
       "    'page': 6,\n",
       "    'total_pages': 6,\n",
       "    'images': [{'bbox': [93.29061889648438,\n",
       "       378.16546630859375,\n",
       "       121.84296417236328,\n",
       "       406.7178039550781],\n",
       "      'context': 'æ¬¢è¿åœ¨ â€œ æœºæ™ºæµ â€ å…¬ä¼—å·åå°å›å¤ â€œccâ€ ï¼ŒåŠ å…¥æœºæ™ºæµå¤§æ¨¡å‹äº¤æµç¾¤ ï¼Œä¸æˆ‘ä»¬ä¸€èµ·æ¢ç´¢  AI  ä¸äººç±»\\næ½œèƒ½çš„æœªæ¥ï¼Œä¸€èµ·å…±èµ´  AI  æµªæ½®ï¼',\n",
       "      'extraction_status': 'success',\n",
       "      'extracted_content': 'The image features a logo that consists of a stylized design with two overlapping shapes in light blue and dark blue, set against a black background. Below the design, there is text in Chinese characters that reads \"æœºå™¨æµ,\" which translates to \"Machine Flow\" in English. \\n\\nIf you need further analysis or specific details, please let me know!'}]}}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/22/25, 8:11 AM è°·æ­Œå‘å¸ƒ SigLIP 2 ï¼šå¤šè¯­è¨€è§†è§‰ - è¯­è¨€ç¼–ç å™¨çš„é©å‘½æ€§è¿›æ­¥\n",
      "<image id=\"003\">\n",
      "The image contains a bar graph comparing various metrics across different datasets and models. Hereâ€™s a structured breakdown of the relevant information:\n",
      "\n",
      "### Datasets/Models:\n",
      "1. **A2D**\n",
      "2. **AOKVQA-DA (val)**\n",
      "3. **AOKVQA-MC (val)**\n",
      "4. **COCO-35L (avg34)**\n",
      "5. **COCO-35L (en)**\n",
      "6. **COCOCap**\n",
      "7. **CountBenchQA**\n",
      "8. **DocVQA (val)**\n",
      "9. **GQA**\n",
      "10. **InfoVQA (val)**\n",
      "11. **NLVR2**\n",
      "12. **NoCaps**\n",
      "13. **OCR-VQA**\n",
      "14. **OKVQA**\n",
      "15. **RefCOCO (testA)**\n",
      "16. **RefCOCO (testB)**\n",
      "17. **RefCOCO+ (val)**\n",
      "18. **RefCOCO+ (testA)**\n",
      "19. **RefCOCO+ (testB)**\n",
      "20. **RefCOCOg (test)**\n",
      "21. **RefCOCOg (val)**\n",
      "22. **ST-VQA (val)**\n",
      "23. **SciCap**\n",
      "24. **ScienceQA**\n",
      "25. **Screen2Words**\n",
      "26. **TallyQA (complex)**\n",
      "27. **TallyQA (simple)**\n",
      "28. **TextCaps**\n",
      "29. **TextVQA (val)**\n",
      "30. **VQA2 (minival)**\n",
      "31. **VizWizVQA (val)**\n",
      "32. **WidgetCap**\n",
      "33. **XM3600 (avg35)**\n",
      "34. **XM3600 (en)**\n",
      "35. **Average**\n",
      "\n",
      "### Metrics:\n",
      "- The bars represent different configurations or models:\n",
      "  - **SigLIP L/16 256px** (light blue)\n",
      "  - **SigLIP L/16 256px** (orange)\n",
      "  - **SigLIP S/400m/14 224px** (green)\n",
      "  - **SigLIP S/400m/14 224px** (dark blue)\n",
      "  - **SigLIP S/400m/14 384px** (yellow)\n",
      "\n",
      "### Observations:\n",
      "- Each dataset/model has a corresponding bar for each configuration, indicating performance metrics (likely accuracy or some evaluation score).\n",
      "- The graph appears to show comparative performance across different models and datasets, with variations in height indicating differences in performance.\n",
      "\n",
      "### Conclusion:\n",
      "This image presents a comparative analysis of various visual question answering (VQA) models across multiple datasets, highlighting their performance metrics in a clear bar graph format.\n",
      "</image>\n",
      "è¡¨2ï¼šSigLIP 2åœ¨å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨è¯­ä¹‰åˆ†å‰²ã€æ·±åº¦ä¼°è®¡å’Œè¡¨é¢æ³•çº¿ä¼°è®¡ä¸Šçš„ä¼˜\n",
      "å¼‚è¡¨ç°ã€‚\n",
      "3. ğŸ”„ å‘åå…¼å®¹æ€§\n",
      "SigLIP 2æ²¿ç”¨äº†SigLIPçš„æ¶æ„è®¾è®¡ï¼Œç”¨æˆ·åªéœ€æ›¿æ¢æ¨¡å‹æƒé‡å’Œtokenizerï¼ˆç°å·²å‡çº§ä¸ºå¤šè¯­è¨€ç‰ˆ\n",
      "æœ¬ï¼‰ï¼Œå³å¯äº«å—åˆ°æ€§èƒ½æå‡ã€‚è¿™ç§è®¾è®¡æå¤§åœ°æ–¹ä¾¿äº†ç°æœ‰ç”¨æˆ·çš„è¿ç§»ã€‚\n",
      "4. ğŸ“ åŸç”Ÿå®½é«˜æ¯”ä¸å¯å˜åˆ†è¾¨ç‡\n",
      "SigLIP 2æ¨å‡ºäº†NaFlexå˜ä½“ï¼Œæ”¯æŒå¤šç§åˆ†è¾¨ç‡å¹¶ä¿æŒå›¾åƒçš„åŸç”Ÿå®½é«˜æ¯”ã€‚è¿™å¯¹äºæ–‡æ¡£ç†è§£ç­‰å¯¹å®½é«˜\n",
      "æ¯”æ•æ„Ÿçš„ä»»åŠ¡å°¤ä¸ºé‡è¦ï¼Œæå‡äº†æ¨¡å‹çš„çµæ´»æ€§å’Œå®ç”¨æ€§ã€‚\n",
      "https://mp.weixin.qq.com/s/Ir1cC_lqF0m2LAeYlHI8og 3/6\n"
     ]
    }
   ],
   "source": [
    "print(result['è°·æ­Œå‘å¸ƒSigLIP 2ï¼šå¤šè¯­è¨€è§†è§‰-è¯­è¨€ç¼–ç å™¨çš„é©å‘½æ€§è¿›æ­¥.pdf'][2]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "\n",
    "def test_single_file(file_path: str):\n",
    "    \"\"\"\n",
    "    Test processing of a single document\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the document file to test\n",
    "    \"\"\"\n",
    "    # API endpoint\n",
    "    url = \"http://localhost:8000/process\"\n",
    "    \n",
    "    # Check file exists\n",
    "    file_path = Path(file_path)\n",
    "    if not file_path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    # logger.info(f\"Testing file: {file_path.name}\")\n",
    "    \n",
    "    # Prepare file for request\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        files = [(\"files\", f)]\n",
    "        \n",
    "        try:\n",
    "            # Send request\n",
    "            # logger.info(\"Sending request to API...\")\n",
    "            response = requests.post(url, files=files)\n",
    "            \n",
    "            # Check response\n",
    "            assert response.status_code == 200, f\"API request failed with status code: {response.status_code}\"\n",
    "            \n",
    "            # Parse results\n",
    "            results = response.json()\n",
    "            return results\n",
    "            \n",
    "            # logger.info(\"Test completed successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            # logger.error(f\"Error during testing: {str(e)}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=test_single_file(\"/Users/frank/PycharmProjects/myOmniDocParser/sampledocs/谷歌发布SigLIP 2：多语言视觉-语言编码器的革命性进步.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'谷歌发布SigLIP 2：多语言视觉-语言编码器的革命性进步.pdf': [{'content': '2/22/25, 8:11 AM 谷歌发布 SigLIP 2 ：多语言视觉 - 语言编码器的革命性进步\\n谷歌发布SigLIP 2：多语言视觉-语言编码器的革命性进步\\nInternLM3等LLM 2025年02月22日 06:48 机智流\\n<image id=\"001\">\\nIt appears that the image is blank or contains no visible content. Therefore, there is no information to extract or analyze. If you have another image or specific content you\\'d like me to look at, please share!\\n</image>\\n作者： InternLM3 等 LLM （内容可能有误，请仔细甄别）\\n全文约  2400  字，预计阅读时间  6  分钟\\n论文链接： https://huggingface.co/papers/2502.14786\\n在人工智能领域，视觉-语言模型（Vision-Language Models, VLMs）的技术进步日新月异。最\\n近，Google DeepMind推出了一款名为SigLIP 2的新多语言视觉-语言编码器。这款模型在原有的\\nSigLIP基础上进行了大幅优化，不仅在核心任务上表现出色，还扩展了多语言支持、密集预测和定\\n位能力。今天，我们将以第三方的视角，为大家详细解读SigLIP 2的亮点和优势。\\n🌟 SigLIP 2是什么？有哪些突破？\\nSigLIP 2是SigLIP的升级版本，它通过融合多种先进技术，包括基于字幕的预训练、自监督损失\\n（如自蒸馏、掩码预测）和在线数据整理，显著提升了性能。相比前代，SigLIP 2在零-shot分\\n类、图像-文本检索以及作为VLMs视觉编码器的迁移性能上都有了明显进步。更令人兴奋的是，它\\n在定位任务和密集预测任务（如分割和深度估计）上也实现了质的飞跃。\\n🔍 SigLIP 2的五大核心优势\\n1. 🚀 强大的多语言支持\\nSigLIP 2不仅在以英语为主的视觉-语言任务中表现出色，还能在多语言基准测试中提供强劲的结\\n果。这使得它能够无缝适应多种语言和文化场景，为全球化应用打开了大门。\\nhttps://mp.weixin.qq.com/s/Ir1cC_lqF0m2LAeYlHI8og 1/6',\n",
       "   'metadata': {'source': '/var/folders/jz/pbm5_77s7951llp98zs6b3d80000gp/T/tmp65eb0on_/谷歌发布SigLIP 2：多语言视觉-语言编码器的革命性进步.pdf',\n",
       "    'page': 1,\n",
       "    'total_pages': 6,\n",
       "    'images': [{'bbox': [77.71660614013672,\n",
       "       103.67327880859375,\n",
       "       517.033447265625,\n",
       "       177.0009002685547],\n",
       "      'context': '🔍 SigLIP 2的五大核心优势\\n1. 🚀 强大的多语言支持',\n",
       "      'extraction_status': 'success',\n",
       "      'extracted_content': \"It appears that the image is blank or contains no visible content. Therefore, there is no information to extract or analyze. If you have another image or specific content you'd like me to look at, please share!\"}]}},\n",
       "  {'content': '2/22/25, 8:11 AM 谷歌发布 SigLIP 2 ：多语言视觉 - 语言编码器的革命性进步\\n<image id=\"002\">\\nThe image presents two bar graphs comparing the performance of three models—SigLIP, SigLIP 2, and mSigLIP—across two tasks: Text to Image Retrieval and Image to Text Retrieval.\\n\\n### Graph Details:\\n\\n1. **Top Graph (Text → Image Retrieval)**:\\n   - **Y-Axis**: Recall (ranging from 0 to 1)\\n   - **X-Axis**: Different languages or categories (e.g., English, Spanish, Filipino, etc.)\\n   - **Bars**:\\n     - **SigLIP**: Represented in blue\\n     - **SigLIP 2**: Represented in orange\\n     - **mSigLIP**: Represented in green\\n   - **Trend**: The recall values for each model vary across different languages, with some languages showing higher recall than others.\\n\\n2. **Bottom Graph (Image → Text Retrieval)**:\\n   - **Y-Axis**: Recall (ranging from 0 to 1)\\n   - **X-Axis**: Same languages or categories as the top graph.\\n   - **Bars**:\\n     - **SigLIP**: Blue\\n     - **SigLIP 2**: Orange\\n     - **mSigLIP**: Green\\n   - **Trend**: Similar to the top graph, the recall values differ across languages, indicating varying performance of the models.\\n\\n### Observations:\\n- Both graphs show a range of recall values, indicating the effectiveness of each model in retrieving relevant information based on the input type (text or image).\\n- The models exhibit different performance levels across various languages, suggesting that some languages may be more challenging for the models than others.\\n\\n### Conclusion:\\nThe graphs effectively illustrate the comparative performance of the three models in two retrieval tasks, highlighting the variability in recall across different languages.\\n</image>\\n图2：SigLIP、SigLIP 2和mSigLIP在Crossmodal-3600上的每种语言图像-文本检索性能对比。\\nSigLIP 2在多语言检索中几乎追平mSigLIP，同时在英语任务上表现更优。\\n2. 🖼 ️  密集特征的提升\\n通过结合自监督损失和基于解码器的损失，SigLIP 2在密集预测任务（如语义分割、深度估计）和\\n定位任务（如指代表达式理解）上表现更佳。这种能力对于需要精细视觉理解的应用至关重要。\\nhttps://mp.weixin.qq.com/s/Ir1cC_lqF0m2LAeYlHI8og 2/6',\n",
       "   'metadata': {'source': '/var/folders/jz/pbm5_77s7951llp98zs6b3d80000gp/T/tmp65eb0on_/谷歌发布SigLIP 2：多语言视觉-语言编码器的革命性进步.pdf',\n",
       "    'page': 2,\n",
       "    'total_pages': 6,\n",
       "    'images': [{'bbox': [77.71660614013672,\n",
       "       27.75,\n",
       "       517.033447265625,\n",
       "       203.60650634765625],\n",
       "      'context': '2. 🖼 ️  密集特征的提升',\n",
       "      'extraction_status': 'success',\n",
       "      'extracted_content': 'The image presents two bar graphs comparing the performance of three models—SigLIP, SigLIP 2, and mSigLIP—across two tasks: Text to Image Retrieval and Image to Text Retrieval.\\n\\n### Graph Details:\\n\\n1. **Top Graph (Text → Image Retrieval)**:\\n   - **Y-Axis**: Recall (ranging from 0 to 1)\\n   - **X-Axis**: Different languages or categories (e.g., English, Spanish, Filipino, etc.)\\n   - **Bars**:\\n     - **SigLIP**: Represented in blue\\n     - **SigLIP 2**: Represented in orange\\n     - **mSigLIP**: Represented in green\\n   - **Trend**: The recall values for each model vary across different languages, with some languages showing higher recall than others.\\n\\n2. **Bottom Graph (Image → Text Retrieval)**:\\n   - **Y-Axis**: Recall (ranging from 0 to 1)\\n   - **X-Axis**: Same languages or categories as the top graph.\\n   - **Bars**:\\n     - **SigLIP**: Blue\\n     - **SigLIP 2**: Orange\\n     - **mSigLIP**: Green\\n   - **Trend**: Similar to the top graph, the recall values differ across languages, indicating varying performance of the models.\\n\\n### Observations:\\n- Both graphs show a range of recall values, indicating the effectiveness of each model in retrieving relevant information based on the input type (text or image).\\n- The models exhibit different performance levels across various languages, suggesting that some languages may be more challenging for the models than others.\\n\\n### Conclusion:\\nThe graphs effectively illustrate the comparative performance of the three models in two retrieval tasks, highlighting the variability in recall across different languages.'}]}},\n",
       "  {'content': '2/22/25, 8:11 AM 谷歌发布 SigLIP 2 ：多语言视觉 - 语言编码器的革命性进步\\n<image id=\"003\">\\nThe image contains a bar graph comparing various metrics across different datasets and models. Here’s a structured breakdown of the relevant information:\\n\\n### Datasets/Models:\\n1. **A2D**\\n2. **AOKVQA-DA (val)**\\n3. **AOKVQA-MC (val)**\\n4. **COCO-35L (avg34)**\\n5. **COCO-35L (en)**\\n6. **COCOCap**\\n7. **CountBenchQA**\\n8. **DocVQA (val)**\\n9. **GQA**\\n10. **InfoVQA (val)**\\n11. **NLVR2**\\n12. **NoCaps**\\n13. **OCR-VQA**\\n14. **OKVQA**\\n15. **RefCOCO (testA)**\\n16. **RefCOCO (testB)**\\n17. **RefCOCO+ (val)**\\n18. **RefCOCO+ (testA)**\\n19. **RefCOCO+ (testB)**\\n20. **RefCOCOg (test)**\\n21. **RefCOCOg (val)**\\n22. **ST-VQA (val)**\\n23. **SciCap**\\n24. **ScienceQA**\\n25. **Screen2Words**\\n26. **TallyQA (complex)**\\n27. **TallyQA (simple)**\\n28. **TextCaps**\\n29. **TextVQA (val)**\\n30. **VQA2 (minival)**\\n31. **VizWizVQA (val)**\\n32. **WidgetCap**\\n33. **XM3600 (avg35)**\\n34. **XM3600 (en)**\\n35. **Average**\\n\\n### Metrics:\\n- The bars represent different configurations or models:\\n  - **SigLIP L/16 256px** (light blue)\\n  - **SigLIP L/16 256px** (orange)\\n  - **SigLIP S/400m/14 224px** (green)\\n  - **SigLIP S/400m/14 224px** (dark blue)\\n  - **SigLIP S/400m/14 384px** (yellow)\\n\\n### Observations:\\n- Each dataset/model has a corresponding bar for each configuration, indicating performance metrics (likely accuracy or some evaluation score).\\n- The graph appears to show comparative performance across different models and datasets, with variations in height indicating differences in performance.\\n\\n### Conclusion:\\nThis image presents a comparative analysis of various visual question answering (VQA) models across multiple datasets, highlighting their performance metrics in a clear bar graph format.\\n</image>\\n表2：SigLIP 2在密集预测任务上的性能，展示了其在语义分割、深度估计和表面法线估计上的优\\n异表现。\\n3. 🔄 向后兼容性\\nSigLIP 2沿用了SigLIP的架构设计，用户只需替换模型权重和tokenizer（现已升级为多语言版\\n本），即可享受到性能提升。这种设计极大地方便了现有用户的迁移。\\n4. 📐 原生宽高比与可变分辨率\\nSigLIP 2推出了NaFlex变体，支持多种分辨率并保持图像的原生宽高比。这对于文档理解等对宽高\\n比敏感的任务尤为重要，提升了模型的灵活性和实用性。\\nhttps://mp.weixin.qq.com/s/Ir1cC_lqF0m2LAeYlHI8og 3/6',\n",
       "   'metadata': {'source': '/var/folders/jz/pbm5_77s7951llp98zs6b3d80000gp/T/tmp65eb0on_/谷歌发布SigLIP 2：多语言视觉-语言编码器的革命性进步.pdf',\n",
       "    'page': 3,\n",
       "    'total_pages': 6,\n",
       "    'images': [{'bbox': [77.71660614013672,\n",
       "       27.75,\n",
       "       517.033447265625,\n",
       "       499.51263427734375],\n",
       "      'context': '3. 🔄 向后兼容性\\n4. 📐 原生宽高比与可变分辨率',\n",
       "      'extraction_status': 'success',\n",
       "      'extracted_content': 'The image contains a bar graph comparing various metrics across different datasets and models. Here’s a structured breakdown of the relevant information:\\n\\n### Datasets/Models:\\n1. **A2D**\\n2. **AOKVQA-DA (val)**\\n3. **AOKVQA-MC (val)**\\n4. **COCO-35L (avg34)**\\n5. **COCO-35L (en)**\\n6. **COCOCap**\\n7. **CountBenchQA**\\n8. **DocVQA (val)**\\n9. **GQA**\\n10. **InfoVQA (val)**\\n11. **NLVR2**\\n12. **NoCaps**\\n13. **OCR-VQA**\\n14. **OKVQA**\\n15. **RefCOCO (testA)**\\n16. **RefCOCO (testB)**\\n17. **RefCOCO+ (val)**\\n18. **RefCOCO+ (testA)**\\n19. **RefCOCO+ (testB)**\\n20. **RefCOCOg (test)**\\n21. **RefCOCOg (val)**\\n22. **ST-VQA (val)**\\n23. **SciCap**\\n24. **ScienceQA**\\n25. **Screen2Words**\\n26. **TallyQA (complex)**\\n27. **TallyQA (simple)**\\n28. **TextCaps**\\n29. **TextVQA (val)**\\n30. **VQA2 (minival)**\\n31. **VizWizVQA (val)**\\n32. **WidgetCap**\\n33. **XM3600 (avg35)**\\n34. **XM3600 (en)**\\n35. **Average**\\n\\n### Metrics:\\n- The bars represent different configurations or models:\\n  - **SigLIP L/16 256px** (light blue)\\n  - **SigLIP L/16 256px** (orange)\\n  - **SigLIP S/400m/14 224px** (green)\\n  - **SigLIP S/400m/14 224px** (dark blue)\\n  - **SigLIP S/400m/14 384px** (yellow)\\n\\n### Observations:\\n- Each dataset/model has a corresponding bar for each configuration, indicating performance metrics (likely accuracy or some evaluation score).\\n- The graph appears to show comparative performance across different models and datasets, with variations in height indicating differences in performance.\\n\\n### Conclusion:\\nThis image presents a comparative analysis of various visual question answering (VQA) models across multiple datasets, highlighting their performance metrics in a clear bar graph format.'}]}},\n",
       "  {'content': '2/22/25, 8:11 AM 谷歌发布 SigLIP 2 ：多语言视觉 - 语言编码器的革命性进步\\n<image id=\"004\">\\nThe image contains a series of graphs comparing different models across various tasks, focusing on performance metrics as a function of sequence length. Here’s a breakdown of the relevant information:\\n\\n### Graph Titles:\\n1. **INet 0-shot**\\n2. **INet v2 0-shot**\\n3. **INet ReaL 0-shot**\\n4. **ObjectNet 0-shot**\\n5. **COCO R@1 ↔ T**\\n6. **TextCaps R@1 ↔ I**\\n7. **TextCaps R@1 ↔ T**\\n8. **HierText R@1 ↔ I**\\n9. **HierText R@1 ↔ T**\\n10. **SciCap R@1 ↔ I**\\n11. **SciCap R@1 ↔ T**\\n12. **Screen2Words R@1 ↔ I**\\n13. **Screen2Words R@1 ↔ T**\\n\\n### Axes:\\n- **X-axis:** Sequence length (values: 64, 256, 576, 784, 1024)\\n- **Y-axis:** Performance metric (percentage, ranging from 0 to 100)\\n\\n### Models:\\n- **SigLIP 2 (NaFlex)**: Represented by a blue line.\\n- **SigLIP 2**: Represented by an orange line.\\n- **ViT**: Indicated by a specific marker style (not detailed in the image).\\n\\n### Additional Markers:\\n- **B/16**: Indicated by a specific marker style (not detailed in the image).\\n- **So400m/16**: Indicated by a specific marker style (not detailed in the image).\\n\\n### Observations:\\n- The graphs show trends in performance for different models as the sequence length increases.\\n- Each graph compares the performance of the models on specific tasks, with varying results based on the model and task.\\n- The performance generally increases with longer sequence lengths, but the rate of increase varies by model and task.\\n\\nThis analysis captures the key elements and relationships presented in the image.\\n</image>\\n图3：NaFlex变体与标准SigLIP 2在不同序列长度下的性能比较，展示了NaFlex在OCR和文档相关\\n任务上的优势。\\n5. 💪 强大的小模型\\n通过主动数据整理技术，SigLIP 2优化了较小模型（如B/16和B/32）的性能。即使在资源受限的\\n场景下，这些小模型也能保持出色的表现。\\n📊 SigLIP 2的性能数据\\nSigLIP 2在多项基准测试中超越了SigLIP和其他开源模型。以下是它在零-shot分类和检索任务中\\n的表现：\\n<image id=\"005\">\\nThe image presents two bar graphs comparing the performance of three models—SigLIP, SigLIP 2, and mSigLIP—across two tasks: Text to Image Retrieval and Image to Text Retrieval.\\n\\n### Graph Details:\\n\\n1. **Top Graph (Text → Image Retrieval)**:\\n   - **Y-Axis**: Recall (ranging from 0 to 1)\\n   - **X-Axis**: Different languages or categories (e.g., English, Spanish, Filipino, etc.)\\n   - **Bars**:\\n     - **SigLIP**: Represented in blue\\n     - **SigLIP 2**: Represented in orange\\n     - **mSigLIP**: Represented in green\\n   - **Trend**: The recall values for each model vary across different languages, with some languages showing higher recall than others.\\n\\n2. **Bottom Graph (Image → Text Retrieval)**:\\n   - **Y-Axis**: Recall (ranging from 0 to 1)\\n   - **X-Axis**: Different languages or categories (e.g., Arabic, Hungarian, English, etc.)\\n   - **Bars**:\\n     - **SigLIP**: Represented in blue\\n     - **SigLIP 2**: Represented in orange\\n     - **mSigLIP**: Represented in green\\n   - **Trend**: Similar to the top graph, the recall values differ across languages, indicating varying performance of the models.\\n\\n### Observations:\\n- The models exhibit different levels of effectiveness in retrieving images based on text and vice versa.\\n- Certain languages may yield higher recall rates, suggesting that the models perform better with specific languages or datasets.\\n\\n### Conclusion:\\nThe graphs provide a comparative analysis of the three models\\' performance in text-to-image and image-to-text retrieval tasks, highlighting the variability in recall across different languages.\\n</image>\\n图4：SigLIP 2与多个基线模型在ImageNet-1k、COCO、Flickr和XM3600上的性能对比。SigLIP\\n2在所有模型规模上均优于SigLIP，尤其在小模型上进步显著。\\nhttps://mp.weixin.qq.com/s/Ir1cC_lqF0m2LAeYlHI8og 4/6',\n",
       "   'metadata': {'source': '/var/folders/jz/pbm5_77s7951llp98zs6b3d80000gp/T/tmp65eb0on_/谷歌发布SigLIP 2：多语言视觉-语言编码器的革命性进步.pdf',\n",
       "    'page': 4,\n",
       "    'total_pages': 6,\n",
       "    'images': [{'bbox': [77.71660614013672,\n",
       "       27.75,\n",
       "       517.033447265625,\n",
       "       293.1570739746094],\n",
       "      'context': '5. 💪 强大的小模型\\n📊 SigLIP 2的性能数据',\n",
       "      'extraction_status': 'success',\n",
       "      'extracted_content': 'The image contains a series of graphs comparing different models across various tasks, focusing on performance metrics as a function of sequence length. Here’s a breakdown of the relevant information:\\n\\n### Graph Titles:\\n1. **INet 0-shot**\\n2. **INet v2 0-shot**\\n3. **INet ReaL 0-shot**\\n4. **ObjectNet 0-shot**\\n5. **COCO R@1 ↔ T**\\n6. **TextCaps R@1 ↔ I**\\n7. **TextCaps R@1 ↔ T**\\n8. **HierText R@1 ↔ I**\\n9. **HierText R@1 ↔ T**\\n10. **SciCap R@1 ↔ I**\\n11. **SciCap R@1 ↔ T**\\n12. **Screen2Words R@1 ↔ I**\\n13. **Screen2Words R@1 ↔ T**\\n\\n### Axes:\\n- **X-axis:** Sequence length (values: 64, 256, 576, 784, 1024)\\n- **Y-axis:** Performance metric (percentage, ranging from 0 to 100)\\n\\n### Models:\\n- **SigLIP 2 (NaFlex)**: Represented by a blue line.\\n- **SigLIP 2**: Represented by an orange line.\\n- **ViT**: Indicated by a specific marker style (not detailed in the image).\\n\\n### Additional Markers:\\n- **B/16**: Indicated by a specific marker style (not detailed in the image).\\n- **So400m/16**: Indicated by a specific marker style (not detailed in the image).\\n\\n### Observations:\\n- The graphs show trends in performance for different models as the sequence length increases.\\n- Each graph compares the performance of the models on specific tasks, with varying results based on the model and task.\\n- The performance generally increases with longer sequence lengths, but the rate of increase varies by model and task.\\n\\nThis analysis captures the key elements and relationships presented in the image.'},\n",
       "     {'bbox': [77.71660614013672,\n",
       "       548.1815185546875,\n",
       "       517.033447265625,\n",
       "       724.0380249023438],\n",
       "      'context': 'SigLIP 2在多项基准测试中超越了SigLIP和其他开源模型。以下是它在零-shot分类和检索任务中\\n的表现：',\n",
       "      'extraction_status': 'success',\n",
       "      'extracted_content': \"The image presents two bar graphs comparing the performance of three models—SigLIP, SigLIP 2, and mSigLIP—across two tasks: Text to Image Retrieval and Image to Text Retrieval.\\n\\n### Graph Details:\\n\\n1. **Top Graph (Text → Image Retrieval)**:\\n   - **Y-Axis**: Recall (ranging from 0 to 1)\\n   - **X-Axis**: Different languages or categories (e.g., English, Spanish, Filipino, etc.)\\n   - **Bars**:\\n     - **SigLIP**: Represented in blue\\n     - **SigLIP 2**: Represented in orange\\n     - **mSigLIP**: Represented in green\\n   - **Trend**: The recall values for each model vary across different languages, with some languages showing higher recall than others.\\n\\n2. **Bottom Graph (Image → Text Retrieval)**:\\n   - **Y-Axis**: Recall (ranging from 0 to 1)\\n   - **X-Axis**: Different languages or categories (e.g., Arabic, Hungarian, English, etc.)\\n   - **Bars**:\\n     - **SigLIP**: Represented in blue\\n     - **SigLIP 2**: Represented in orange\\n     - **mSigLIP**: Represented in green\\n   - **Trend**: Similar to the top graph, the recall values differ across languages, indicating varying performance of the models.\\n\\n### Observations:\\n- The models exhibit different levels of effectiveness in retrieving images based on text and vice versa.\\n- Certain languages may yield higher recall rates, suggesting that the models perform better with specific languages or datasets.\\n\\n### Conclusion:\\nThe graphs provide a comparative analysis of the three models' performance in text-to-image and image-to-text retrieval tasks, highlighting the variability in recall across different languages.\"}]}},\n",
       "  {'content': '2/22/25, 8:11 AM 谷歌发布 SigLIP 2 ：多语言视觉 - 语言编码器的革命性进步\\n🌍 文化多样性与公平性\\nSigLIP 2不仅在性能上更进一步，还在文化包容性和公平性上迈出了重要一步。它通过训练数据中\\n的多语言支持和去偏技术，在地理多样化的物体分类和地理定位任务中表现出色，同时显著降低了\\n表示偏差。\\n<image id=\"006\">\\nThe image presents two bar graphs comparing performance metrics across different models in two scenarios: \"10-shot\" and \"0-shot.\"\\n\\n### Graph Details:\\n\\n#### 10-shot Graph (Left):\\n- **X-axis Categories**:\\n  - Dollar Street\\n  - GeoDE (country)\\n  - GeoDE (region)\\n  \\n- **Y-axis**: Performance metric (values range from 0 to 40)\\n\\n- **Bars**:\\n  - **SigLIP B/16**: Light blue\\n  - **SigLIP 2 B/16**: Dark blue\\n  - **SigLIP L/16**: Light brown\\n  - **SigLIP 2 L/16**: Dark brown\\n\\n#### 0-shot Graph (Right):\\n- **X-axis Categories**:\\n  - Dollar Street\\n  - GLDv2\\n  - GeoDE\\n  \\n- **Y-axis**: Performance metric (values range from 0 to 100)\\n\\n- **Bars**:\\n  - **SigLIP B/16**: Light blue\\n  - **SigLIP 2 B/16**: Dark blue\\n  - **SigLIP L/16**: Light brown\\n  - **SigLIP 2 L/16**: Dark brown\\n\\n### Observations:\\n- In the **10-shot** scenario, the highest performance is observed for **GeoDE (region)** with **SigLIP 2 L/16**.\\n- In the **0-shot** scenario, **GeoDE** shows the highest performance, particularly with **SigLIP B/16**.\\n- The performance metrics differ significantly between the two scenarios, indicating varying effectiveness of the models based on the shot type.\\n\\n### Conclusion:\\nThe graphs illustrate the comparative performance of different models under varying conditions, highlighting the effectiveness of specific configurations in both 10-shot and 0-shot learning scenarios.\\n</image>\\n图5：SigLIP 2在Dollar Street、GeoDE和GLDv2上的10-shot和0-shot准确率，展示了其在文化\\n多样性任务上的优异表现。\\n<image id=\"007\">\\nThe image is a bar chart displaying data related to \"Representation bias\" for different categories labeled as follows:\\n\\n### Categories:\\n1. **SigLIP B/16**\\n   - Color: Light Blue\\n   - Value: Approximately 30\\n\\n2. **SigLIP 2 B/16**\\n   - Color: Dark Blue\\n   - Value: Approximately 20\\n\\n3. **SigLIP L/16**\\n   - Color: Light Brown\\n   - Value: Approximately 25\\n\\n4. **SigLIP 2 L/16**\\n   - Color: Dark Brown\\n   - Value: Approximately 10\\n\\n### X-Axis:\\n- Label: **Representation bias**\\n- Range: 0 to 35\\n\\n### Y-Axis:\\n- Categories listed vertically.\\n\\n### Observations:\\n- The highest representation bias is observed in **SigLIP B/16**.\\n- The lowest representation bias is in **SigLIP 2 L/16**.\\n- The chart visually compares the representation bias across the four categories.\\n</image>\\n图6：不同模型的表示偏差对比，SigLIP 2显著降低了偏差，提升了模型的公平性。\\n🎯 总结\\nSigLIP 2的发布标志着多语言视觉-语言编码器的一次重要飞跃。它在性能、兼容性、多语言支持\\n和公平性上的全面提升，使其成为视觉-语言任务的理想选择。对于研究人员和开发者来说，\\nhttps://mp.weixin.qq.com/s/Ir1cC_lqF0m2LAeYlHI8og 5/6',\n",
       "   'metadata': {'source': '/var/folders/jz/pbm5_77s7951llp98zs6b3d80000gp/T/tmp65eb0on_/谷歌发布SigLIP 2：多语言视觉-语言编码器的革命性进步.pdf',\n",
       "    'page': 5,\n",
       "    'total_pages': 6,\n",
       "    'images': [{'bbox': [78.36552429199219,\n",
       "       132.22572326660156,\n",
       "       516.384521484375,\n",
       "       371.67608642578125],\n",
       "      'context': '的多语言支持和去偏技术，在地理多样化的物体分类和地理定位任务中表现出色，同时显著降低了\\n表示偏差。',\n",
       "      'extraction_status': 'success',\n",
       "      'extracted_content': 'The image presents two bar graphs comparing performance metrics across different models in two scenarios: \"10-shot\" and \"0-shot.\"\\n\\n### Graph Details:\\n\\n#### 10-shot Graph (Left):\\n- **X-axis Categories**:\\n  - Dollar Street\\n  - GeoDE (country)\\n  - GeoDE (region)\\n  \\n- **Y-axis**: Performance metric (values range from 0 to 40)\\n\\n- **Bars**:\\n  - **SigLIP B/16**: Light blue\\n  - **SigLIP 2 B/16**: Dark blue\\n  - **SigLIP L/16**: Light brown\\n  - **SigLIP 2 L/16**: Dark brown\\n\\n#### 0-shot Graph (Right):\\n- **X-axis Categories**:\\n  - Dollar Street\\n  - GLDv2\\n  - GeoDE\\n  \\n- **Y-axis**: Performance metric (values range from 0 to 100)\\n\\n- **Bars**:\\n  - **SigLIP B/16**: Light blue\\n  - **SigLIP 2 B/16**: Dark blue\\n  - **SigLIP L/16**: Light brown\\n  - **SigLIP 2 L/16**: Dark brown\\n\\n### Observations:\\n- In the **10-shot** scenario, the highest performance is observed for **GeoDE (region)** with **SigLIP 2 L/16**.\\n- In the **0-shot** scenario, **GeoDE** shows the highest performance, particularly with **SigLIP B/16**.\\n- The performance metrics differ significantly between the two scenarios, indicating varying effectiveness of the models based on the shot type.\\n\\n### Conclusion:\\nThe graphs illustrate the comparative performance of different models under varying conditions, highlighting the effectiveness of specific configurations in both 10-shot and 0-shot learning scenarios.'},\n",
       "     {'bbox': [121.19405364990234,\n",
       "       441.75909423828125,\n",
       "       473.5559997558594,\n",
       "       653.9549560546875],\n",
       "      'context': '图5：SigLIP 2在Dollar Street、GeoDE和GLDv2上的10-shot和0-shot准确率，展示了其在文化\\n多样性任务上的优异表现。',\n",
       "      'extraction_status': 'success',\n",
       "      'extracted_content': 'The image is a bar chart displaying data related to \"Representation bias\" for different categories labeled as follows:\\n\\n### Categories:\\n1. **SigLIP B/16**\\n   - Color: Light Blue\\n   - Value: Approximately 30\\n\\n2. **SigLIP 2 B/16**\\n   - Color: Dark Blue\\n   - Value: Approximately 20\\n\\n3. **SigLIP L/16**\\n   - Color: Light Brown\\n   - Value: Approximately 25\\n\\n4. **SigLIP 2 L/16**\\n   - Color: Dark Brown\\n   - Value: Approximately 10\\n\\n### X-Axis:\\n- Label: **Representation bias**\\n- Range: 0 to 35\\n\\n### Y-Axis:\\n- Categories listed vertically.\\n\\n### Observations:\\n- The highest representation bias is observed in **SigLIP B/16**.\\n- The lowest representation bias is in **SigLIP 2 L/16**.\\n- The chart visually compares the representation bias across the four categories.'}]}},\n",
       "  {'content': '2/22/25, 8:11 AM 谷歌发布 SigLIP 2 ：多语言视觉 - 语言编码器的革命性进步\\nSigLIP 2不仅提供了更高的性能，还带来了更广泛的应用可能性。\\n期待SigLIP 2在开源社区中激发更多创新火花！🔥\\n-- 完 --\\n机智流 推荐 阅读：\\n1.\\xa0 掌握如何搭建高效的大模型任务流（二）：如何用链式流程提升 AI 处理能力？\\n2.\\xa0 掌握如何搭建高效的大模型任务流（一）：LangChain任务流构建\\n3.\\xa0 揭秘LangChain记忆模块：万字长文详解四种Conversation Memory，助力AI对话更聪\\n明！\\n4.\\xa0 万字长文！手把手带你上手基于LangChain及Qwen大模型的开发与应用\\n欢迎在 “ 机智流 ” 公众号后台回复 “cc” ，加入机智流大模型交流群 ，与我们一起探索  AI  与人类\\n潜能的未来，一起共赴  AI  浪潮！\\n<image id=\"008\">\\nThe image features a logo that consists of a stylized design with two overlapping shapes in light blue and dark blue, set against a black background. Below the design, there is text in Chinese characters that reads \"机器流,\" which translates to \"Machine Flow\" in English. \\n\\nIf you need further analysis or specific details, please let me know!\\n</image>\\n机智流\\n共赴 AI 时代浪潮~\\n476篇原创内容\\n公众号\\nhttps://mp.weixin.qq.com/s/Ir1cC_lqF0m2LAeYlHI8og 6/6',\n",
       "   'metadata': {'source': '/var/folders/jz/pbm5_77s7951llp98zs6b3d80000gp/T/tmp65eb0on_/谷歌发布SigLIP 2：多语言视觉-语言编码器的革命性进步.pdf',\n",
       "    'page': 6,\n",
       "    'total_pages': 6,\n",
       "    'images': [{'bbox': [93.29061889648438,\n",
       "       378.16546630859375,\n",
       "       121.84296417236328,\n",
       "       406.7178039550781],\n",
       "      'context': '欢迎在 “ 机智流 ” 公众号后台回复 “cc” ，加入机智流大模型交流群 ，与我们一起探索  AI  与人类\\n潜能的未来，一起共赴  AI  浪潮！',\n",
       "      'extraction_status': 'success',\n",
       "      'extracted_content': 'The image features a logo that consists of a stylized design with two overlapping shapes in light blue and dark blue, set against a black background. Below the design, there is text in Chinese characters that reads \"机器流,\" which translates to \"Machine Flow\" in English. \\n\\nIf you need further analysis or specific details, please let me know!'}]}}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/22/25, 8:11 AM 谷歌发布 SigLIP 2 ：多语言视觉 - 语言编码器的革命性进步\n",
      "<image id=\"003\">\n",
      "The image contains a bar graph comparing various metrics across different datasets and models. Here’s a structured breakdown of the relevant information:\n",
      "\n",
      "### Datasets/Models:\n",
      "1. **A2D**\n",
      "2. **AOKVQA-DA (val)**\n",
      "3. **AOKVQA-MC (val)**\n",
      "4. **COCO-35L (avg34)**\n",
      "5. **COCO-35L (en)**\n",
      "6. **COCOCap**\n",
      "7. **CountBenchQA**\n",
      "8. **DocVQA (val)**\n",
      "9. **GQA**\n",
      "10. **InfoVQA (val)**\n",
      "11. **NLVR2**\n",
      "12. **NoCaps**\n",
      "13. **OCR-VQA**\n",
      "14. **OKVQA**\n",
      "15. **RefCOCO (testA)**\n",
      "16. **RefCOCO (testB)**\n",
      "17. **RefCOCO+ (val)**\n",
      "18. **RefCOCO+ (testA)**\n",
      "19. **RefCOCO+ (testB)**\n",
      "20. **RefCOCOg (test)**\n",
      "21. **RefCOCOg (val)**\n",
      "22. **ST-VQA (val)**\n",
      "23. **SciCap**\n",
      "24. **ScienceQA**\n",
      "25. **Screen2Words**\n",
      "26. **TallyQA (complex)**\n",
      "27. **TallyQA (simple)**\n",
      "28. **TextCaps**\n",
      "29. **TextVQA (val)**\n",
      "30. **VQA2 (minival)**\n",
      "31. **VizWizVQA (val)**\n",
      "32. **WidgetCap**\n",
      "33. **XM3600 (avg35)**\n",
      "34. **XM3600 (en)**\n",
      "35. **Average**\n",
      "\n",
      "### Metrics:\n",
      "- The bars represent different configurations or models:\n",
      "  - **SigLIP L/16 256px** (light blue)\n",
      "  - **SigLIP L/16 256px** (orange)\n",
      "  - **SigLIP S/400m/14 224px** (green)\n",
      "  - **SigLIP S/400m/14 224px** (dark blue)\n",
      "  - **SigLIP S/400m/14 384px** (yellow)\n",
      "\n",
      "### Observations:\n",
      "- Each dataset/model has a corresponding bar for each configuration, indicating performance metrics (likely accuracy or some evaluation score).\n",
      "- The graph appears to show comparative performance across different models and datasets, with variations in height indicating differences in performance.\n",
      "\n",
      "### Conclusion:\n",
      "This image presents a comparative analysis of various visual question answering (VQA) models across multiple datasets, highlighting their performance metrics in a clear bar graph format.\n",
      "</image>\n",
      "表2：SigLIP 2在密集预测任务上的性能，展示了其在语义分割、深度估计和表面法线估计上的优\n",
      "异表现。\n",
      "3. 🔄 向后兼容性\n",
      "SigLIP 2沿用了SigLIP的架构设计，用户只需替换模型权重和tokenizer（现已升级为多语言版\n",
      "本），即可享受到性能提升。这种设计极大地方便了现有用户的迁移。\n",
      "4. 📐 原生宽高比与可变分辨率\n",
      "SigLIP 2推出了NaFlex变体，支持多种分辨率并保持图像的原生宽高比。这对于文档理解等对宽高\n",
      "比敏感的任务尤为重要，提升了模型的灵活性和实用性。\n",
      "https://mp.weixin.qq.com/s/Ir1cC_lqF0m2LAeYlHI8og 3/6\n"
     ]
    }
   ],
   "source": [
    "print(result['谷歌发布SigLIP 2：多语言视觉-语言编码器的革命性进步.pdf'][2]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
